{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where we will specifically put all code and answers for our second analysis question\n",
    "\n",
    "## Question: If S&P is increasing or decreasing, which demographic is most closely related to the S&P performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m By\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import random\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()\n",
    "browser.get(\"https://www.bls.gov/charts/employment-situation/civilian-unemployment-rate.htm\")\n",
    "browser.maximize_window()\n",
    "wait = WebDriverWait(browser, 15)\n",
    "show_table_button = wait.until(\n",
    "    EC.element_to_be_clickable((By.LINK_TEXT, \"Show table\"))\n",
    ")\n",
    "\n",
    "show_table_button.click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "table = browser.find_element(By.TAG_NAME, \"table\")\n",
    "rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "month_years, total_rates, men_rates, women_rates, teen_rates, white_rates, black_rates, asian_rates, latino_rates = ([] for _ in range(9))\n",
    "\n",
    "for row in rows[1:]:\n",
    "    cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "    if len(cols) == 8:\n",
    "        th = row.find_element(By.TAG_NAME, \"th\")\n",
    "        month_year = th.find_element(By.CLASS_NAME, \"sub0\").text\n",
    "        month_years.append(month_year)\n",
    "            total_rates.append(cols[0].text)\n",
    "        men_rates.append(cols[1].text)\n",
    "        women_rates.append(cols[2].text)\n",
    "        teen_rates.append(cols[3].text)\n",
    "        white_rates.append(cols[4].text)\n",
    "        black_rates.append(cols[5].text)\n",
    "        asian_rates.append(cols[6].text)\n",
    "        latino_rates.append(cols[7].text)\n",
    "\n",
    "unemployment_df = pd.DataFrame({\n",
    "    \"Date\": month_years,\n",
    "    \"Total Rate\": total_rates,\n",
    "    \"Male Rate\": men_rates,\n",
    "    \"Female Rate\": women_rates,\n",
    "    \"Teen Rate\": teen_rates,\n",
    "    \"White Rate\": white_rates,\n",
    "    \"Black Rate\": black_rates,\n",
    "    \"Asian Rate\": asian_rates,\n",
    "    \"Hispanic Rate\": latino_rates\n",
    "})\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "print(unemployment_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_df = pd.read_csv(\"spy.csv\")\n",
    "sp500_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- Load your data (if not already loaded) ---\n",
    "# unemployment_df = pd.read_csv('your_unemployment_file.csv')\n",
    "# sp500_df = pd.read_csv('your_sp500_file.csv')\n",
    "\n",
    "# --- 1. Convert 'Date' columns to datetime ---\n",
    "unemployment_df['Date'] = pd.to_datetime(unemployment_df['Date'], format='%Y-%m-%d')\n",
    "sp500_df['Date'] = pd.to_datetime(sp500_df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "# --- 2. Clean unemployment rate columns (remove % and convert to float) ---\n",
    "rate_cols = [col for col in unemployment_df.columns if col != 'Date']\n",
    "\n",
    "for col in rate_cols:\n",
    "    unemployment_df[col] = pd.to_numeric(unemployment_df[col].astype(str).str.replace('%', ''), errors='coerce')\n",
    "\n",
    "# --- 3. Merge unemployment and S&P 500 data on 'Date' ---\n",
    "merged_df = pd.merge(unemployment_df, sp500_df[['Date', 'Close']], on='Date')\n",
    "\n",
    "# --- 4. Sort by date and create S&P trend direction ---\n",
    "merged_df.sort_values('Date', inplace=True)\n",
    "merged_df['SP_Trend'] = merged_df['Close'].diff().apply(lambda x: 'Increase' if x > 0 else 'Decrease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=merged_df, x='Date', y='Close', label='S&P 500')\n",
    "sns.lineplot(data=merged_df, x='Date', y='Female Rate', label='Female Unemployment')\n",
    "sns.lineplot(data=merged_df, x='Date', y='Male Rate', label='Male Unemployment')\n",
    "sns.lineplot(data=merged_df, x='Date', y='Teen Rate', label='Teen Unemployment')\n",
    "sns.lineplot(data=merged_df, x='Date', y='White Rate', label='White Unemployment')\n",
    "sns.lineplot(data=merged_df, x='Date', y='Black Rate', label='Black Unemployment')\n",
    "sns.lineplot(data=merged_df, x='Date', y='Asian Rate', label='Asian Unemployment')\n",
    "sns.lineplot(data=merged_df, x='Date', y='Hispanic Rate', label='Hispanic Unemployment')\n",
    "plt.title('S&P 500 vs Unemployment Rates Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Male Rate', 'Female Rate', 'Teen Rate', 'White Rate', 'Black Rate', 'Asian Rate', 'Hispanic Rate']\n",
    "X = merged_df[features]\n",
    "y = merged_df['SP_Trend']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "importances = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_cols = [col for col in unemployment_df.columns if col != 'Date']\n",
    "correlations = merged_df[rate_cols + ['Close']].corr()['Close'].drop('Close')\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=correlations.index, y=correlations.values, palette='viridis')\n",
    "plt.title('Correlation of Unemployment Rates with S&P 500 Close')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create new columns based on whether the rate exceeds the median for each demographic group\n",
    "merged_df['High_Teen_Unemployment'] = merged_df['Teen Rate'] > merged_df['Teen Rate'].median()\n",
    "merged_df['High_Male_Unemployment'] = merged_df['Male Rate'] > merged_df['Male Rate'].median()\n",
    "merged_df['High_Female_Unemployment'] = merged_df['Female Rate'] > merged_df['Female Rate'].median()\n",
    "merged_df['High_White_Unemployment'] = merged_df['White Rate'] > merged_df['White Rate'].median()\n",
    "merged_df['High_Black_Unemployment'] = merged_df['Black Rate'] > merged_df['Black Rate'].median()\n",
    "merged_df['High_Asian_Unemployment'] = merged_df['Asian Rate'] > merged_df['Asian Rate'].median()\n",
    "merged_df['High_Hispanic_Unemployment'] = merged_df['Hispanic Rate'] > merged_df['Hispanic Rate'].median()\n",
    "\n",
    "# Ensure that 'SP_Trend' is categorical\n",
    "merged_df['SP_Trend'] = merged_df['SP_Trend'].astype('category')\n",
    "\n",
    "# List of demographic columns and corresponding \"High\" Unemployment columns\n",
    "demographic_columns = [\n",
    "    ('Teen', 'High_Teen_Unemployment'),\n",
    "    ('Male', 'High_Male_Unemployment'),\n",
    "    ('Female', 'High_Female_Unemployment'),\n",
    "    ('White', 'High_White_Unemployment'),\n",
    "    ('Black', 'High_Black_Unemployment'),\n",
    "    ('Asian', 'High_Asian_Unemployment'),\n",
    "    ('Hispanic', 'High_Hispanic_Unemployment')\n",
    "]\n",
    "# Perform the chi-square test for each rate and print the results\n",
    "for rate, high_column in demographic_columns:\n",
    "    contingency = pd.crosstab(merged_df['SP_Trend'], merged_df[high_column])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "    \n",
    "    print(f\"Results for {rate} Unemployment:\")\n",
    "    print(f\"Chi-square statistic: {chi2}\")\n",
    "    print(f\"P-value: {p}\")\n",
    "    print(f\"Degrees of freedom: {dof}\")\n",
    "    print(f\"Expected frequencies:\\n{expected}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in features:\n",
    "    merged_df[f'{col}_lag1'] = merged_df[col].shift(1)\n",
    "\n",
    "# Drop rows with NaNs introduced by lagging\n",
    "merged_df_lagged = merged_df.dropna(subset=[f'{col}_lag1' for col in features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run new classificaiton \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "lagged_features = [f'{col}_lag1' for col in features]\n",
    "X_lag = merged_df_lagged[lagged_features]\n",
    "y_lag = merged_df_lagged['SP_Trend']\n",
    "\n",
    "#set test at 20%\n",
    "X_train_lag, X_test_lag, y_train_lag, y_test_lag = train_test_split(X_lag, y_lag, test_size=0.2, random_state=42)\n",
    "\n",
    "#NEED TO CHECK!!\n",
    "#to predict continuous values, may need to use regression model (like RandomForestRegressor) instead of a classifier\n",
    "#not confident tho\n",
    "model_lag = RandomForestClassifier(random_state=42)\n",
    "model_lag.fit(X_train_lag, y_train_lag)\n",
    "\n",
    "y_pred_lag = model_lag.predict(X_test_lag)\n",
    "print(classification_report(y_test_lag, y_pred_lag))\n",
    "\n",
    "# Feature importances for lagged features\n",
    "importances_lag = pd.Series(model_lag.feature_importances_, index=lagged_features).sort_values(ascending=False)\n",
    "print(importances_lag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a continuous target \n",
    "\n",
    "# Calculate S&P percent change and drop missing values\n",
    "merged_df['SP_Change'] = merged_df['Close'].pct_change()\n",
    "merged_df = merged_df.dropna(subset=['SP_Change'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, y_train)\n",
    "print(\"Logistic Coefficients:\\n\", pd.Series(log_model.coef_[0], index=features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve (for binary classificaiton) since we are predicting an increase or decrease\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Encode target to binary\n",
    "le = LabelEncoder()\n",
    "y_bin = le.fit_transform(y)  # 'Decrease'=0, 'Increase'=1\n",
    "\n",
    "# Split and fit model again with encoded target\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(X, y_bin, test_size=0.2, random_state=42)\n",
    "model_bin = RandomForestClassifier(random_state=42)\n",
    "model_bin.fit(X_train_bin, y_train_bin)\n",
    "y_proba = model_bin.predict_proba(X_test_bin)[:, 1]  # Probability of 'Increase'\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_bin, y_proba)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc_score(y_test_bin, y_proba):.2f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Predicting S&P Trend\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df['SP_Trend'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Original predicted probabilities\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Flipped probabilities\n",
    "y_proba_flipped = 1 - y_proba\n",
    "\n",
    "# Recompute AUC for flipped predictions\n",
    "roc_auc_flipped = roc_auc_score(y_test, y_proba_flipped)\n",
    "print(f\"Flipped AUC: {roc_auc_flipped:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print new AUC gra\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Step 1: Encode the target variable as binary ---\n",
    "# 'Increase' = 1 (positive class), 'Decrease' = 0\n",
    "merged_df['SP_Trend'] = merged_df['SP_Trend'].map({'Increase': 1, 'Decrease': 0})\n",
    "\n",
    "# --- Step 2: Select features and optionally create lagged features ---\n",
    "features = ['Male Rate', 'Female Rate', 'Teen Rate', 'White Rate', 'Black Rate', 'Asian Rate', 'Hispanic Rate']\n",
    "\n",
    "# Create lagged features\n",
    "for col in features:\n",
    "    merged_df[f'{col}_lag1'] = merged_df[col].shift(1)\n",
    "\n",
    "# Drop rows with NaNs introduced by lagging\n",
    "merged_df_lagged = merged_df.dropna(subset=[f'{col}_lag1' for col in features])\n",
    "\n",
    "# Define feature set and target\n",
    "lagged_features = [f'{col}_lag1' for col in features]\n",
    "X = merged_df_lagged[lagged_features]\n",
    "y = merged_df_lagged['SP_Trend']\n",
    "# --- Step 3: Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Step 4: Train Random Forest Classifier ---\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- Step 5: Predict and Evaluate ---\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# --- Step 6: AUC and ROC Curve ---\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"AUC: {roc_auc:.2f}\")\n",
    "\n",
    "# Check flipped AUC\n",
    "roc_auc_flipped = roc_auc_score(y_test, 1 - y_proba)\n",
    "print(f\"Flipped AUC: {roc_auc_flipped:.2f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - S&P Trend Prediction')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explanation of the Change \n",
    "\n",
    "Metric:                         Value:\tMeaning:\n",
    "AUC  \t                        0.41\tPoor ranking of true class 1\n",
    "Flipped AUC\t                    0.59\tSlightly better ranking when 1 and 0 are reversed\n",
    "Precision/Recall for class 1\t0.74\tModel is biased toward class 1 and gets most right\n",
    "Precision/Recall for class 0\t0.25\tModel struggles to identify minority class (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_pred_lag = model_lag.predict(X_test_lag)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(y_test_lag, y_pred_lag),\n",
    "    display_labels=['Increase', 'Decrease']  \n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "disp.plot(cmap=\"Blues\", values_format='d')\n",
    "plt.title(\"Confusion Matrix - S&P Trend Prediction (with Lags)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot feature importances\n",
    "importances = pd.Series(model_lag.feature_importances_, index=lagged_features).sort_values()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "importances.plot(kind='barh', color='skyblue')\n",
    "plt.title(\"Feature Importances - Random Forest (Original + Lagged)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_lagged)\n",
    "\n",
    "# Fit logistic regression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_scaled, y_lagged)\n",
    "\n",
    "# Coefficients and direction of influence\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': lagged_features,\n",
    "    'Coefficient': log_reg.coef_[0]\n",
    "}).sort_values(by='Coefficient')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Coefficient', y='Feature', data=coef_df, palette='coolwarm')\n",
    "plt.title(\"Logistic Regression Coefficients (S&P Trend Prediction)\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
