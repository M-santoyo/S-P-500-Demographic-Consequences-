{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is where we will specifically put all code and answers for our second analysis question\n",
    "\n",
    "## Question: If S&P is increasing or decreasing, which demographic is most closely related to the S&P performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is all inputs needed to run our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report, mean_squared_error, r2_score, roc_curve, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our web scraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Chrome()\n",
    "browser.get(\"https://www.bls.gov/charts/employment-situation/civilian-unemployment-rate.htm\")\n",
    "browser.maximize_window()\n",
    "wait = WebDriverWait(browser, 15)\n",
    "show_table_button = wait.until(\n",
    "    EC.element_to_be_clickable((By.LINK_TEXT, \"Show table\"))\n",
    ")\n",
    "\n",
    "show_table_button.click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "table = browser.find_element(By.TAG_NAME, \"table\")\n",
    "rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "\n",
    "month_years, total_rates, men_rates, women_rates, teen_rates, white_rates, black_rates, asian_rates, latino_rates = ([] for _ in range(9))\n",
    "\n",
    "for row in rows[1:]:\n",
    "    cols = row.find_elements(By.TAG_NAME, \"td\")\n",
    "    if len(cols) == 8:\n",
    "        th = row.find_element(By.TAG_NAME, \"th\")\n",
    "        month_year = th.find_element(By.CLASS_NAME, \"sub0\").text\n",
    "        month_years.append(month_year)\n",
    "        total_rates.append(cols[0].text)\n",
    "        men_rates.append(cols[1].text)\n",
    "        women_rates.append(cols[2].text)\n",
    "        teen_rates.append(cols[3].text)\n",
    "        white_rates.append(cols[4].text)\n",
    "        black_rates.append(cols[5].text)\n",
    "        asian_rates.append(cols[6].text)\n",
    "        latino_rates.append(cols[7].text)\n",
    "        unemployment_df = pd.DataFrame({\n",
    "    \"Date\": month_years,\n",
    "    \"Total Rate\": total_rates,\n",
    "    \"Male Rate\": men_rates,\n",
    "    \"Female Rate\": women_rates,\n",
    "    \"Teen Rate\": teen_rates,\n",
    "    \"White Rate\": white_rates,\n",
    "    \"Black Rate\": black_rates,\n",
    "    \"Asian Rate\": asian_rates,\n",
    "    \"Hispanic Rate\": latino_rates\n",
    "})\n",
    "\n",
    "browser.quit()\n",
    "\n",
    "print(unemployment_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the reading of our S&P 500 data and then merged onto the unemployment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_df = pd.read_csv(\"spy.csv\")\n",
    "sp500_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unemployment_df['Date'] = pd.to_datetime(unemployment_df['Date'], errors='coerce')\n",
    "sp500_df['Date'] = pd.to_datetime(sp500_df['Date'], errors='coerce')\n",
    "\n",
    "rate_cols = [col for col in unemployment_df.columns if col != 'Date']\n",
    "\n",
    "for col in rate_cols:\n",
    "    unemployment_df[col] = pd.to_numeric(unemployment_df[col].astype(str).str.replace('%', ''), errors='coerce')\n",
    "\n",
    "# Merging data\n",
    "merged_df = pd.merge(unemployment_df, sp500_df[['Date', 'Close']], on='Date')\n",
    "\n",
    "merged_df.sort_values('Date', inplace=True)\n",
    "merged_df['SP_Trend'] = merged_df['Close'].diff().apply(lambda x: 'Increase' if x > 0 else 'Decrease')\n",
    "\n",
    "# Check for any issues in the Date column\n",
    "print(merged_df['Date'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates a multi-line time series plot that visualizes the S&P 500 closing price alongside various unemployment rates (by demographic group) over time. Each line represents a different variable, allowing for visual comparison of market performance and unemployment trends across groups, all plotted against the same time axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=merged_df, x='Date', y='Close', label='S&P 500')\n",
    "sns.lineplot(data=merged_df, x='Date', y='Female Rate', label='Female Unemployment')\n",
    "sns.lineplot(data=merged_df, x='Date', y='Male Rate', label='Male Unemployment')\n",
    "sns.lineplot(data=merged_df, x='Date', y='Teen Rate', label='Teen Unemployment')\n",
    "sns.lineplot(data=merged_df, x='Date', y='White Rate', label='White Unemployment')\n",
    "sns.lineplot(data=merged_df, x='Date', y='Black Rate', label='Black Unemployment')\n",
    "sns.lineplot(data=merged_df, x='Date', y='Asian Rate', label='Asian Unemployment')\n",
    "sns.lineplot(data=merged_df, x='Date', y='Hispanic Rate', label='Hispanic Unemployment')\n",
    "plt.title('S&P 500 vs Unemployment Rates Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code trains a Random Forest classifier to predict SP_Trend (e.g., an indicator of S&P 500 market direction) using various demographic unemployment rates as features. It splits the data into training and test sets, fits the model, evaluates its performance using a classification report, and displays the importance of each feature based on how much it contributes to the modelâ€™s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Male Rate', 'Female Rate', 'Teen Rate', 'White Rate', 'Black Rate', 'Asian Rate', 'Hispanic Rate']\n",
    "X = merged_df[features]\n",
    "y = merged_df['SP_Trend']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "importances = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates the correlation between each unemployment rate (excluding the 'Date' column) and the S&P 500 closing price, then visualizes the results as a bar plot. Each bar represents how strongly a specific unemployment rate is linearly related to the S&P 500 close, making it easy to compare the strength and direction of these relationships across demographic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_cols = [col for col in unemployment_df.columns if col != 'Date']\n",
    "correlations = merged_df[rate_cols + ['Close']].corr()['Close'].drop('Close')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=correlations.index, y=correlations.values, palette='viridis')\n",
    "plt.title('Correlation of Unemployment Rates with S&P 500 Close')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code creates binary indicators for each demographic group based on whether their unemployment rate is above the median, then conducts a chi-square test of independence to assess whether there is a statistically significant association between high unemployment (by group) and S&P 500 trend (SP_Trend). For each demographic group, it prints the chi-square statistic, p-value, degrees of freedom, and expected frequencies, helping determine whether stock market movements are linked to elevated unemployment in specific populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['High_Teen_Unemployment'] = merged_df['Teen Rate'] > merged_df['Teen Rate'].median()\n",
    "merged_df['High_Male_Unemployment'] = merged_df['Male Rate'] > merged_df['Male Rate'].median()\n",
    "merged_df['High_Female_Unemployment'] = merged_df['Female Rate'] > merged_df['Female Rate'].median()\n",
    "merged_df['High_White_Unemployment'] = merged_df['White Rate'] > merged_df['White Rate'].median()\n",
    "merged_df['High_Black_Unemployment'] = merged_df['Black Rate'] > merged_df['Black Rate'].median()\n",
    "merged_df['High_Asian_Unemployment'] = merged_df['Asian Rate'] > merged_df['Asian Rate'].median()\n",
    "merged_df['High_Hispanic_Unemployment'] = merged_df['Hispanic Rate'] > merged_df['Hispanic Rate'].median()\n",
    "\n",
    "merged_df['SP_Trend'] = merged_df['SP_Trend'].astype('category')\n",
    "\n",
    "demographic_columns = [\n",
    "    ('Teen', 'High_Teen_Unemployment'),\n",
    "    ('Male', 'High_Male_Unemployment'),\n",
    "    ('Female', 'High_Female_Unemployment'),\n",
    "    ('White', 'High_White_Unemployment'),\n",
    "    ('Black', 'High_Black_Unemployment'),\n",
    "    ('Asian', 'High_Asian_Unemployment'),\n",
    "    ('Hispanic', 'High_Hispanic_Unemployment')\n",
    "]\n",
    "\n",
    "for rate, high_column in demographic_columns:\n",
    "    contingency = pd.crosstab(merged_df['SP_Trend'], merged_df[high_column])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency)\n",
    "    \n",
    "    print(f\"Results for {rate} Unemployment:\")\n",
    "    print(f\"Chi-square statistic: {chi2}\")\n",
    "    print(f\"P-value: {p}\")\n",
    "    print(f\"Degrees of freedom: {dof}\")\n",
    "    print(f\"Expected frequencies:\\n{expected}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in features:\n",
    "    merged_df[f'{col}_lag1'] = merged_df[col].shift(1)\n",
    "\n",
    "merged_df_lagged = merged_df.dropna(subset=[f'{col}_lag1' for col in features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses lagged versions of unemployment rate features (i.e., values from the previous time period) to train a Random Forest classifier that predicts SP_Trend. It splits the data into training and testing sets, fits the model, evaluates prediction performance with a classification report, and prints the importance of each lagged feature, helping identify which past unemployment rates are most influential in forecasting S&P 500 movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged_features = [f'{col}_lag1' for col in features]\n",
    "X_lag = merged_df_lagged[lagged_features]\n",
    "y_lag = merged_df_lagged['SP_Trend']\n",
    "\n",
    "X_train_lag, X_test_lag, y_train_lag, y_test_lag = train_test_split(X_lag, y_lag, test_size=0.2, random_state=42)\n",
    "\n",
    "model_lag = RandomForestClassifier(random_state=42)\n",
    "model_lag.fit(X_train_lag, y_train_lag)\n",
    "\n",
    "y_pred_lag = model_lag.predict(X_test_lag)\n",
    "print(classification_report(y_test_lag, y_pred_lag))\n",
    "\n",
    "importances_lag = pd.Series(model_lag.feature_importances_, index=lagged_features).sort_values(ascending=False)\n",
    "print(importances_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['SP_Change'] = merged_df['Close'].pct_change()\n",
    "merged_df = merged_df.dropna(subset=['SP_Change'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fits a logistic regression model using the training data (X_train, y_train) to predict SP_Trend, then prints the model's learned coefficients for each unemployment rate feature. These coefficients indicate the direction and strength of each variable's influence on the likelihood of the S&P 500 trending upward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model.fit(X_train, y_train)\n",
    "print(\"Logistic Coefficients:\\n\", pd.Series(log_model.coef_[0], index=features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code encodes the categorical target SP_Trend into binary form, trains a Random Forest classifier, and evaluates its probabilistic predictions using an ROC curve. It plots the True Positive Rate against the False Positive Rate at various thresholds and displays the AUC score, which summarizes the model's overall ability to distinguish between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_bin = le.fit_transform(y)\n",
    "\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(X, y_bin, test_size=0.2, random_state=42)\n",
    "model_bin = RandomForestClassifier(random_state=42)\n",
    "model_bin.fit(X_train_bin, y_train_bin)\n",
    "y_proba = model_bin.predict_proba(X_test_bin)[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test_bin, y_proba)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc_score(y_test_bin, y_proba):.2f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Predicting S&P Trend\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df['SP_Trend'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates the predicted probabilities for the positive class using a trained model, then flips those probabilities to represent the negative class. It computes and prints the ROC AUC score based on the flipped probabilities, which evaluates how well the model distinguishes the negative class (0) instead of the positive class (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_proba_flipped = 1 - y_proba\n",
    "\n",
    "roc_auc_flipped = roc_auc_score(y_test, y_proba_flipped)\n",
    "print(f\"Flipped AUC: {roc_auc_flipped:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code preprocesses the data by mapping SP_Trend to binary values, then creates lagged versions of various unemployment rate features to predict S&P 500 trend (SP_Trend). It trains a Random Forest classifier on these lagged features, evaluates the model's performance using a classification report and AUC (Area Under the Curve) score, and computes an alternative AUC score after flipping the predicted probabilities. Finally, it visualizes the ROC curve to assess how well the model discriminates between the two trends (Increase vs. Decrease) of the S&P 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['SP_Trend'] = merged_df['SP_Trend'].map({'Increase': 1, 'Decrease': 0})\n",
    "\n",
    "features = ['Male Rate', 'Female Rate', 'Teen Rate', 'White Rate', 'Black Rate', 'Asian Rate', 'Hispanic Rate']\n",
    "\n",
    "for col in features:\n",
    "    merged_df[f'{col}_lag1'] = merged_df[col].shift(1)\n",
    "\n",
    "merged_df_lagged = merged_df.dropna(subset=[f'{col}_lag1' for col in features])\n",
    "\n",
    "lagged_features = [f'{col}_lag1' for col in features]\n",
    "X = merged_df_lagged[lagged_features]\n",
    "y = merged_df_lagged['SP_Trend']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"AUC: {roc_auc:.2f}\")\n",
    "\n",
    "roc_auc_flipped = roc_auc_score(y_test, 1 - y_proba)\n",
    "print(f\"Flipped AUC: {roc_auc_flipped:.2f}\")\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve - S&P Trend Prediction')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric:                         Value:\tMeaning:\n",
    "AUC  \t                        0.41\tPoor ranking of true class 1\n",
    "Flipped AUC\t                    0.59\tSlightly better ranking when 1 and 0 are reversed\n",
    "Precision/Recall for class 1\t0.74\tModel is biased toward class 1 and gets most right\n",
    "Precision/Recall for class 0\t0.25\tModel struggles to identify minority class (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code generates a confusion matrix to evaluate the performance of the lagged version of the Random Forest classifier. It computes the confusion matrix for the predicted (y_pred_lag) and actual (y_test_lag) SP_Trend values, then displays the matrix with labels for \"Increase\" and \"Decrease.\" The plot uses a blue color map to visualize the matrix, which shows how well the model classified the S&P 500 trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lag = model_lag.predict(X_test_lag)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(y_test_lag, y_pred_lag),\n",
    "    display_labels=['Increase', 'Decrease']  \n",
    ")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "disp.plot(cmap=\"Blues\", values_format='d')\n",
    "plt.title(\"Confusion Matrix - S&P Trend Prediction (with Lags)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code visualizes the feature importances from the lagged Random Forest model by plotting a horizontal bar chart. The bar chart shows the relative importance of each lagged feature in predicting the S&P 500 trend, with the features sorted by importance. The chart helps identify which lagged unemployment rate features contribute most to the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = pd.Series(model_lag.feature_importances_, index=lagged_features).sort_values()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "importances.plot(kind='barh', color='skyblue')\n",
    "plt.title(\"Feature Importances - Random Forest (Original + Lagged)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code scales the lagged feature data using StandardScaler, then fits a logistic regression model to predict the S&P 500 trend (y_lagged). It extracts the model's coefficients for each feature and visualizes them in a horizontal bar plot, where the length of each bar indicates the strength and direction of the feature's effect on the outcome. The plot helps interpret which lagged features are most influential in predicting the S&P 500 trend."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
